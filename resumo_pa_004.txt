Problema de negócio: uma empresa de seguros possui uma base de clientes que utiliza um determinado serviço de seguro de saúde anual. Agora, a empresa quer oferecer um novo serviço para novos clientes, um tipo de seguro de carro. Para isso, é necessário prever quais são as 2 mil melhores propensões de compra de possíveis novos clientes em uma base de dados de 127 mil possíveis novos clientes através de uma análise da base de clientes já existente.

Sprints:
1. Planejamento da solução e coleta dos dados.
2. Descrição dos dados e pesquisa de negócio (o que é um cross-sell).
3. Análise exploratória dos dados e relatório de insights.
4. Preparação dos dados.
5. Modelagem dos dados e tunagem de hiperparâmetros.
6. Métricas de negócio e tradução da performance do modelo para resultados de negócio.
7. Deploy do modelo em produção.
8. Acesso aos dados em produção (usando o Google Sheets).
9. Apresentação para uma equipe de negócio e escrita de artigo.

Readme do Github:
1. Problema de negócio.
2. Premissas de negócio e definição de termos principais.
3. Passo a passo da solução (apresentar a estratégia).
4. Top 3 insights da EDA.
5. Modelagem e performance após o cross-validation.
6. Resultados de negócio.
7. Conclusão, lições aprendidas e próximos passos.

Como usar o Git para o PA 004:
- GitLab da comunidade: https://gitlab.com/datascience-community/pa004_health_insurance_cross_sell
- git clone https://gitlab.com/datascience-community/pa004_health_insurance_cross_sell.git
- git checkout -b pa004_nome_sobrenome
- git push origin pa004_nome_sobrenome

Database: Postgres 12.0 AWS
- HOST: comunidade-ds-postgres.c50pcakiuwi3.us-east-1.rds.amazonaws.com
- PORT: 5432.
- Database: comunidadedsdb.
- Username: member.
- Password: cdspa.
- Procedural Language (PL) SQL: baseada em funções/triggers. Caso não usado com sabedoria, gera muitos custos, pois acumula muitos dados. Portanto, é melhor processar em Python e deixar a PL/SQL para consultas mais difíceis.

Como instalar o WSL2 para emular o Linux no Windows:
- Digitar na barra de comando do terminal: wsl --install para instalar e wsl --shutdown para finalizar.
- New UNIX username: brunohmr
- New password: bhmr2210

Password Postgres: Health@2210
Port: 5432

Fonte dos dados no kaggle: https://www.kaggle.com/datasets/anmolkumar/health-insurance-cross-sell-prediction.

Cookiecutter: automatizador de criação de repositórios pelo terminal (pip install cookiecutter). Olhar live 017.

------------------------------------------------------------------------------------------------------------------------------------------
Coleta dos dados

1. Instalar o PSQL (https://www.postgresql.org/download/windows/). Após a instalação, poderá encontrar o terminal pesquisando no menu iniciar por SQL Shell (Psql) e a aplicação pesquisando por pgAdmin4.
* Para ler o manual, digitar no terminal: man postgres.
* Barra escapada (\) no terminal pula a linha sem iniciar um novo comando, muito útil para quando acaba a tela do terminal.

2. Passar as credenciais no terminal (SQL Shell) para conectar ao banco de dados Postgres:
psql -h comunidade-ds-postgres.c50pcakiuwi3.us-east-1.rds.amazonaws.com \
-p 5432 \
-d comunidadedsdb \
-U member \
-W cdspa
* Ao pedir a senha de novo, digitar novamente cdspa e dar enter.
* Pode utilizar o Dbeaver para fazer a conexão também.

3. Listar os Schemas do database digitando o comando \dn no terminal. Listar as tabelas do schema digitando \dt+ pa004.* Utilizar o asterisco para listar todas as tabelas do Schema. O sinal de soma (+) serve para listar o tamanho das tabelas. Para desconectar do banco de dados digitar no terminal: \q.

4. Quando usar o Dbeaver ou outra IDE: fazer as consultas rápidas e validações antes de processar em Python. Pode exportar a tabela como .csv clicando com o botão direito do mouse sobre ela e indo na opção 'Export data'.

5. SQL no Python (script 'conexao_postgres').

Dúvida ao monitor: como mascarar as credenciais no Python.

------------------------------------------------------------------------------------------------------------------------------------------
Modelo de negócio de uma empresa de seguros

Exemplo prático:
- Como funciona uma seguradora: o usuário contrata um serviço de seguro de saúde no valor anual de R$5.000,00 e o serviço de seguro assegura R$200.000,00 ao usuário em caso de algum problema de saúde que esteja incluso no contrato.
- Como a seguradora ganha dinheiro: ela calcula a probabilidade das pessoas ficarem doentes durante o ano. Por isso que os valores do seguro variam de acordo com as características do usuário (idade, habitos alimentares, estilo de vida...).

Lucro: número de tickets*valor do ticket - custos.
1. Ticket:
- Aumentar o valor do ticket. Oferecer diferentes tickets (fazer Upsell).
2. Número de vendas:
- Aumentar o número de clientes
- Aumentar a frequência de compra do cliente. Para isso, é necessário oferecer mais produtos.
3. Custos:
- Custo da operação. Nas startups, era comum primeiro não ter lucro e sim escala (vai de acordo com o desejo dos investidores). Porém, após alguns casos de divergência nos dados de faturamento de startups, os investidores ficaram mais cautelosos e hoje as startups não pensam mais apenas em escala, mas também em atingir o break-even.
- Custo de produção do produto e CAC.

Exemplo hipotético do cálculo da Margem de Lucro: 
- Número de clientes: 100
- Valor da venda: R$20,00
- Custos: R$500,00
- Faturamento: 100*20 = R$2.000,00
- Lucro: 2000-500 = R$1.500,00
* Margem de lucro - Meigarom: 100*20/500 = 2000/500 = 4
* Margem de lucro - DSA: 1500/(20*100) = 1500/2000 = 0,75 ou 75%
- A ideia é, além de aumentar o faturamento, reduzir o custo, a fim de aumentar a margem de lucro.

Cross-Sell: oferecer serviços/produtos complementares ou relacionados com base nos interesses do cliente ao adquirir um produto. No projeto, os clientes já possuem um serviço de seguro adquirido e o estudo que será feito tem a motivação de captar a propensão de compra de serviços/produtos complementares.

A estratégia do Cross-Sell:
1. Encontrar potenciais clientes interessados em adquirir um novo produto com base em pesquisas através da seleção de uma amostra de clientes. Com base nos dados coletados dos clientes que demonstraram interesse, analisá-los e encontrar um padrão é fundamental.
2. Restrição de negócio: redução de custo ou limite da quantidade instalada (neste caso, limite de ligações). Com base nas probabilidades de compra e nas classes atribuídas aos clientes, é possível definir a quantidade percentual da base alcançada e a quantidade percentual de compradores interessados, conforme exemplo da figura 'propensao_compra'.

Planejamento da solução:
1. Divisão da base de treino em treino e validação (305.000 para treino e 76.000 para validação) e treina o modelo.
2. Gera as previsões com as probabilidades e classifica com 0 e 1.
3. Cria as duas colunas de % da base e % de interessados com base nas previsões e traça o gráfico.
4. Pega os dados de teste e faz as previsões com o modelo.
5. Utiliza a curva gerada na estapa 3. e compara com as previsões (nos dados de teste não terão as classes).

Como controlar o Overfitting quando a diferença na performance não for tão grande:
1. Acompanhar o desempenho modelo dia a dia.
2. Fazer previsões com menos dados.

------------------------------------------------------------------------------------------------------------------------------------------
Etapa de análise dos dados

Descrição dos atributos:
- id: identificador único do cliente.
- Gender: gênero do cliente.
- Age: idade do cliente.
- Driving_License: 0 quando o cliente não tem licença para dirigir, 1 quando o cliente tem licença para dirigir.
- Region_Code: código único de localização do cliente.
- Previously_Insured: 0 quando o cliente não tem seguro de veículo, 1 quando o cliente já tem um seguro de veículo.
- Vehicle_Age: ano do veículo.
- Vehicle_Damage: 0 quando o cliente não teve seu carro danificado, 1 quando o cliente já teve seu carro danificado.
- Annual_Premium: o montante que o cliente precisa pagar como premium num ano.
- Policy_Sales_Channel: código anônimo para o canal de divulgação para o cliente, ou seja, agentes diferentes, por correio, por telefone, pessoalmente, etc.
- Vintage: período, em dias, desde que o cliente possui contrato com a empresa.
- Response: variável resposta, sendo 0 caso o cliente não tenha interesse na contratação e 1 caso tenha interesse.

Tipos das variáveis:
- Numéricas: id, age, annual_premium e vintage.
- Categóricas: gender, driving_license, region_code, previously_insured, vehicle_age, vehicle_damage, policy_sales_channel e response.

Principais conclusões da análise estatística descritiva:
- Não há NAs e nem outliers aparentes.
- 12% dos clientes possuem interesse em um serviço de seguro de carros.

Engenharia de atributos:
- Uma ideia proposta na Live 023 foi de criar duas novas colunas, uma delas relacionando o preço do seguro (annual_premium) com a idade (age) e a outra com a quantidade de dias (vintage).
- Outra ideia foi criar outras duas novas colunas, uma delas dividindo o annual_premium do cliente pela média do annual_premium para a região do cliente e a outra para o canal de divulgação do cliente, para ver se o valor do cliente está abaixo ou acima da média.

Lista de hipóteses:
1. gender: mulheres possuem mais interesse em adquirir o seguro do que os homens, em proporção. Falsa.
2. driving_license: clientes com licença possuem mais interesse em adquirir o seguro do que clientes sem licença, em proporção. Verdadeira.
3. previously_insured: clientes que já possuem outro contrato de seguro com a empresa possuem mais interesse em adquirir o seguro do que clientes que não possuem outro contrato de seguro com a empresa, em proporção. Verdadeira.
4. vehicle_age: clientes com carros mais novos possuem mais interesse em adquirir o seguro do que clientes com carros mais novos, em proporção. Falsa.
5. vehicle_damage: clientes que já tiveram o carro danificado possuem mais interesse em adquirir o seguro do que clientes que nunca tiveram o carro danificado, em proporção. Verdadeira.

Análise exploratória dos dados (EDA):
- Foi realizada a análise univariada com a contagem total dos dados (scatterplot da contagem total).
- Foi realizada a análise bivariada com a contagem dos dados em relação a variável resposta (scatterplot da contagem de interessados e de não interessados).
- Foi realizada a análise bivariada com o percentual de interesse dos clientes (scatterplot do percentual de interessados).
- Para as variáveis com poucos valores distintos foram realizados apenas crosstabs.
- Para as variáveis com muitos valores distintos (mais de 3) foram realizados crosstabs e scatterplots.

Relatório do SweetViz:
- Mostra as associações (correlações) entre as variáveis categóricas e numéricas no canto direito da visualização da análise de cada variável ou clicando no botão Associations.
- Mostra a distribuição da contagem de valores totais para cada variável através dos bins (auto, 5, 15 ou 30).
- Relaciona cada variável com a porcentagem de interesse da variável resposta através da linha contínua com os marcadores.
- Mostra análise estatística descritiva para cada variável.

Principais conclusões da EDA:
1. id: não possui nenhuma relação explícita com a variável resposta.
2. age: a maior quantidade de clientes da base possui menos de 30 anos (entre 21 e 25 anos). Porém, a maior parte dos interessados, em proporção (valores acima de 20%), está entre os 33 e 48 anos.
3. region_code: possui pouca relação explícita com a variável resposta. Destacam-se as regiões 5, 19, 28 e 38 com mais de 15% de proporção de interesse.
4. annual_premium: possui pouca relação explícita com a variável resposta. Seguros entre 200k e 300k possuem um percentual de interesse um pouco maior, mas a amostragem é muito pequena. Cerca de 17% dos clientes teriam um seguro anual no valor de $2.630,00 (valor mais baixo).
5. policy_sales_channel: o canal com maior contagem de clientes é o canal com menor proporção de interesse (canal 152). O canal 156 possui uma amostra relevante de clientes (mais de 10 mil) e um percentual de interesse maior que 20%.
6. vintage: possui pouca relação explícita com a variável resposta.

------------------------------------------------------------------------------------------------------------------------------------------
Etapa de ciência dos dados

Preparação dos dados:
- annual_premium: se retirar o valor de $2.630,00 da análise, o qual possui uma contagem imensamente superior aos demais, verifica-se que a distribuição dos dados é semelhante a uma normal, podendo aplicar a normalização. É possível visualizar isso também através do relatório do SweetViz com 30 bins.
- age: observando a distribuição dos dados pelo relatório do SweetViz com 30 bins, é possível observar uma distribuição semelhante a uma Poisson. Deste modo, aplica-se a técnica de reescala.
- vintage: distribuição totalmente aleatória. Aplicar reescala.
- As demais variáveis são categóricas. Portanto, aplicam-se encodings.
- Variáveis que já estão divididas em 0 e 1 possuem um Label Encoder implícito, não necessitando transformação.
- Caso usar o Target Encoding, assume-se que as proporções calculadas nos dados de treino se mantém para os dados de teste, pois nos dados de teste não se tem a variável resposta para comparar.

Seleção de features:
- Se for separar os dados somente em treino e validação, precisa separar antes de fazer a preparação dos dados para que os dados de validação não vejam as transformações que serão feitas nos dados de treino.
- Pode acontecer algum caso em que um valor contido nas colunas 'policy_sales_channel' e 'region_code' do conjunto de validação não possua nenhuma correspondência no conjunto de treino. Se isso acontecer, devido ao Target/Frequency Encoding, não será possível mapear tal valor, de modo que o conjunto de validação ficaria com NaN nestes registros. Caso isso aconteça, preenche-se estes registros com 0.

Modelagem:
- Como é um problema de ordenação, a performance deve medir a qualidade da ordenação do modelo e não a performance da classificação.
- Bootstrap sample: amostra com repetição (podem haver dois registros iguais). Para que o número de possibilidades seja ilimitado, introduzindo variabilidade nos dados. Conceito utilizado no processo de construção da Random Forest.
- Search space: cada dado bootstrap pode ter menos colunas, para que assim também aumente a variabilidade e não selecione sempre o mesmo atributo. É definido pelo parâmetro 'max_features' (sugerido raíz de n atributos).
- Ensemble de modelos: quando faz uma união de vários modelos em que cada um vota para decidir a classificação da variável resposta. Funciona como a Random Forest, porém com modelos diferentes.
- max_estimators da Random Forest: depende muito do range observado na etapa de estatística descritiva, pois quanto maior o range maior a variabilidade, podendo aumentar o número de estimadores. Define a quantidade de árvores e de recortes no espaço de dados.
- max_depth: define o crescimento da árvore. Número de elementos na folha: quanto mais elementos na folha, menor a profundidade da árvore e menor a quantidade de recortes no espaço, reduzindo o viés.

Balanceamento de classes:
1. class_weight: dar peso para as classes para balanceá-las. Peso 1 para a classe majoritária e peso (quantidade de amostras da classe majoritária dividido pela quantidade de amostras da classe minoritária) para a classe minoritária. O algoritmo da Logistic Regression já possui a opção de balanceamento de classes via class_weight imbutida.
2. Under Sampling: quando há muitos exemplos da classe minoritária. Pode utilizar o Random Undersampling ou o Nearmiss Undersampling (semelhante ao KNN, mantém as classes majoritárias cujas distâncias para as classes minoritárias é a menor).
3. Problema do Oversampling: pode overfittar o modelo.
4. Problema do Undersampling: pode perder algum comportamento específico dos dados. Para solucionar este problema, pode fazer uma EDA para a classe majoritária e reduzí-la retirando amostras que não possuam um comportamento que faça sentido.

Curvas de Rankeamento: simular cenários e tomar decisões. Reportar para o time de negócio.
- Curva ROC: para cada threshold, pega um valor de FPR (eixo x) e um valor de TPR (eixo y) e traça uma curva. Com base nos requisitos do negócio, define-se através da curva qual é o melhor threshold e a taxa de erro. Por exemplo: em um problema de classificação de classes onde uma classe é gato e a outra é cachorro, não há uma importância maior entre gato ou cachorro.
- Curva Cumulative Gain: X% da base de clientes, ordenados pela probabilidade de compra (predict_proba), contém Y% de todos os interessados no novo produto.
- Curva Lift: indica quantas vezes o modelo (curva cumulative gain) é melhor que o baseline (curva linear).

Métricas de avaliação:
- Top K: ordenação dos clientes de acordo com o predict_proba.
- Precision Top K: contar quantas previsões foram acertadas até 'k' clientes e dividir pelo número de previsões realizadas até 'k'.
- Recall Top K: contar quantas previsões foram acertadas até 'k' clientes e dividir pelo número total de exemplos verdadeiros.
- O mais utilizado é a previsão.

Exemplos de problemas de Learn to Rank (rankeamento):
- Ordenação de vitrine: dado uma lista de produtos, quais são os mais relevantes que devem aparecer no topo da vitrine? Em casos onde não há nenhuma informação prévia sobre o cliente.
- Sistemas de recomendação: qual é a relevância de cada produto oferecido ao cliente? Em casos onde já existem informações prévias sobre o cliente.
- Motores de busca: os resultados exibidos em uma pesquisa a partir de uma busca são relevantes?

Viés e Variância:
- Viés: representado pela falta de flexibilidade do modelo. Exemplo: um modelo de regressão linear, por ser representado por uma reta, irá possuir um alto viés quando os dados não apresentarem um comportamento linear.
- Variância: modelos mais flexíveis normalmente irão errar muito as previsões de dados nunca vistos, pois eles conseguem 'fitar' muito bem os dados de treino. Logo, qualquer dado que cair longe dos dados de treino irão passar longe das previsões do modelo, apresentando uma alta variância.

Ideia do Lucas (monitor da comunidade DS): fez uma classe que dava o threshold baseado na diferença entre o preço do seguro e o custo. Então, se o lucro fosse menor ou maior o threshold ia variando conforme a alteração de um ou de outro.

Precisão e Recall:
- True positive (TP): o modelo previu como 1 e era 1.
- True negative (TN): o modelo previu como 0 e era 0.
- False positive (FP): o modelo previu como 1 e era 0.
- True positive (FN): o modelo previu como 0 e era 1.
- Precision: TP / (TP + FP) ou previsões positivas acertadas / (todas as previsões positivas realizadas)
- Recall: TP / (TP + FN) ou previsões positivas acertadas / (todos os valores reais positivos)

------------------------------------------------------------------------------------------------------------------------------------------
ETL

Programação orientada a objeto:
- __init__: função construtora. Seta parâmetros fixos da classe. Toda vez que a classe for instanciada (criada uma cópia), a primeira função a ser chamada sera a __init__.
- Formação de um objeto: objeto = classe(valores dos atributos da função construtora da classe).
- self.: indica que a variável que vem após o ponto pertence à determinada classe.
- O criador do código desenvolve uma API para não dar acesso a todo o código-fonte do seu projeto. Exemplo: se um usuário externo quiser acessar uma predição de um código de um projeto de ciência de dados que você desenvolveu, ele deve o parâmetro especificado pela API e em troca o código irá retornar a predição ao usuário, sem a necessidade de expor todo o código.
- setters: definem a função, mas não retornam nada.
- getters: retornam a função, mas não possuem definição.
- Funções Thunder: com underscore, tipo o __init__.
- super().: permite guardar variáveis dentro da classe pai.
- Diagrama UML: conexões entre integrantes e pais. Sistema de microserviços de APIs que acessam modelos de ciência de dados em diferentes bancos de dados.

Modelo em produção:
- Quando é feita uma requisição pelo browser do usuário para, por exemplo, acessar um site de uma empresa de e-commerce através da URL, no fundo o que está acontecendo é que o browser está acessando o end-point da empresa e ela está retornando a página HTML do site.
- API Gateway: primeiro endpoint. Onde está o cache. Toda vez que é feita uma requisição, primeiro é acessado um banco de dados em memória, como o Reddis por exemplo, para que a recuperação das informações ocorra de maneira mais rápida.

Google Sheets/Planilhas:
- É possível editar os avisos dos erros. Por exemplo: se eu escrevi um dado errado, do tipo string, em uma coluna que só aceita valores numéricos, o modelo não irá conseguir realizar a predição e mostrará um erro. Porém, a mensagem de erro do Google Scripts é um HTML desorganizado. Sendo assim, o ideal seria editar a imagem para que os erros se tornem mais intuitivos ao usuário.
- O código no Google Scripts não está realizando corretamente a preparação dos dados preparados com o MinMaxScaler e com o StandardScaler da biblioteca Scikit-Learn. O ideal seria montar duas funções que retornassem estas preparações sem utilizar o Scikit-Learn para testar.
- Precisei criar duas funções específicas para a Standardization e para o MinMaxScaler para funcionar o google scripts.

Ideias para os próximos ciclos:
- Combinar a lista ordenada com uma lista de sobrevivência: a lista de sobrevivência pega o cliente ordenado e diz quantas ligações ou quantos contatos deverão ser feitos com aquele cliente para que o negócio se concretize. Ou seja, não quer dizer que o cliente ordenado em primeiro irá converter logo no primeiro contato.

Dica de como explicar o projeto:
- Explicar o que você fez e o que você esperava que acontecesse fazendo aquilo. Depois, dizer se o que você esperava foi concretizado ou não e por que.
